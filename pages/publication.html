<h2 style="margin-bottom: 10px; font-size: 32px; font-weight: 700; color: #222;">Publications</h2>
<!-- <p style="color: #666; margin-bottom: 40px; font-size: 15px;">A selection of my research work in Human-Computer Interaction and Accessibility.</p> -->

<div class="publication-list">
  
  <div class="publication-item">
    <div class="publication-image">
      <img src="images/ICON.png" alt="ICON" onerror="this.src='https://via.placeholder.com/250x180?text=StepSense'">
    </div>
    <div class="publication-content">
      <h3>Aligning Visual Context in Instructional Videos for Physical Task Assistance: Effectiveness, Attribution, and Feasibility</h3>
      <div class="authors">Yayuan Li*, <span style="font-weight: bold; border-bottom: 2px solid #FF6900;">Chenglin Li</span>*, Jingying Wang, Filippos Bellos, Anhong Guo†, Jason J. Corso†</div>
      <div class="venue">Full Paper Under Review</div>
      <div class="abstract">
        Performing unfamiliar physical tasks from instructional videos often demands substantial cognitive effort. A key 
        but underexplored challenge is the misalignment between the visual context of an instructional video and the user's 
        physical environment. We first conducted a within-subjects study (N=16) to compare commonly shared online instructional 
        videos with in-context videos—those that closely match the user's physical context. Results show that in-context videos 
        significantly improve task completion and reduce cognitive load, and inform four critical dimensions of visual context: 
        task object appearance, task object layout, viewpoint, and background. Based on these findings, we conducted a second 
        between-subjects ablation study (N=40) to quantify their individual contributions to task performance and outcomes. Finally, 
        we discuss the feasibility of scaling in-context instructional videos in the era of generative AI, highlighting both 
        opportunities and limitations.
      </div>
      <div class="publication-links">
      </div>
    </div>
  </div>

  <div class="publication-item">
    <div class="publication-image">
      <img src="images/RAVEN.png" alt="RAVEN" onerror="this.src='https://via.placeholder.com/250x180?text=ICON'">
    </div>
    <div class="publication-content">
      <h3>RAVEN: Realtime Accessibility in Virtual ENvironments for Blind and Low-Vision People</h3>
      <div class="authors">Xinyun Cao, Kexin Phyllis Ju, <span style="font-weight: bold; border-bottom: 2px solid #FF6900;">Chenglin Li</span>, Venkatesh Potluri, Dhruv Jain</div>
      <div class="venue">Full Paper Under Review, ASSETS 25 Demo (Best Demo Award), CHI 25 Late Breaking Work</div>
      <div class="abstract">
        In this work, we present RAVEN, a system that responds to query or modification prompts from BLV users to 
        improve the accessibility of a 3D virtual scene at runtime. We evaluated the system with eight BLV people, 
        uncovering key insights into the strengths and shortcomings of generative AI-driven accessibility in virtual 
        3D environments, pointing to promising results as well as challenges related to system reliability and user trust.
      </div>
      <div class="publication-links">
        <a href="https://arxiv.org/abs/2510.06573" class="pub-link" data-type="pdf">Full Paper</a>
        <a href="https://dl.acm.org/doi/10.1145/3663547.3759725" class="pub-link" data-type="pdf">ASSETS 25 Demo</a>
        <a href="https://dl.acm.org/doi/10.1145/3706599.3720265" class="pub-link" data-type="pdf">CHI 25 LBW</a>
        <a href="https://www.youtube.com/watch?v=uQi0RdGHA0s" class="pub-link" data-type="website">Video</a>


      </div>
    </div>
  </div>

</div>
